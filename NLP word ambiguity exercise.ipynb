{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9b51ab9-d940-483d-8229-e91709192804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After a messy couple of months, the deal between Elon Musk and Twitter finally closed last week. And as his first order of business, Elon Musk walked into the company with a physical sink. On Twitter, he posted a video of this stunt with the caption: “Entering Twitter HQ – let that sink in!” And just like that, Musk’s authority over the popular social media platform begins.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"After a messy couple of months, the deal between Elon Musk and Twitter finally closed last week. And as his first order of business, Elon Musk walked into the company with a physical sink. On Twitter, he posted a video of this stunt with the caption: “Entering Twitter HQ – let that sink in!” And just like that, Musk’s authority over the popular social media platform begins.\"\n",
    "text\n",
    "# what does the word \"sink\" mean? \n",
    "# how about `let that sink in`?\n",
    "# what will the word be classified as?\n",
    "# source https://www.forbes.com/sites/qai/2022/11/05/elon-musk-twitter-sink-whats-elon-up-to-this-time/?sh=76b39e167b1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9cee4b-c221-4373-88a3-26e5c8a61d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06c3235-884a-43e5-9ecf-6743ac998d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization:\n",
      "['After', 'a', 'messy', 'couple', 'of', 'months', ',', 'the', 'deal', 'between', 'Elon', 'Musk', 'and', 'Twitter', 'finally', 'closed', 'last', 'week', '.', 'And', 'as', 'his', 'first', 'order', 'of', 'business', ',', 'Elon', 'Musk', 'walked', 'into', 'the', 'company', 'with', 'a', 'physical', 'sink', '.', 'On', 'Twitter', ',', 'he', 'posted', 'a', 'video', 'of', 'this', 'stunt', 'with', 'the', 'caption', ':', '“', 'Entering', 'Twitter', 'HQ', '–', 'let', 'that', 'sink', 'in', '!', '”', 'And', 'just', 'like', 'that', ',', 'Musk', '’', 's', 'authority', 'over', 'the', 'popular', 'social', 'media', 'platform', 'begins', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "\n",
    "print(\"\\nTokenization:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5802c024-b9b4-4874-9867-fe6292159cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d0a4d47-8b2b-4152-a2cb-646d8a1c908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemming:\n",
      "['after', 'a', 'messi', 'coupl', 'of', 'month', ',', 'the', 'deal', 'between', 'elon', 'musk', 'and', 'twitter', 'final', 'close', 'last', 'week', '.', 'and', 'as', 'hi', 'first', 'order', 'of', 'busi', ',', 'elon', 'musk', 'walk', 'into', 'the', 'compani', 'with', 'a', 'physic', 'sink', '.', 'on', 'twitter', ',', 'he', 'post', 'a', 'video', 'of', 'thi', 'stunt', 'with', 'the', 'caption', ':', '“', 'enter', 'twitter', 'hq', '–', 'let', 'that', 'sink', 'in', '!', '”', 'and', 'just', 'like', 'that', ',', 'musk', '’', 's', 'author', 'over', 'the', 'popular', 'social', 'media', 'platform', 'begin', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming (using Porter Stemmer)\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "print(\"\\nStemming:\")\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc2e9fef-64d6-4c64-93ea-f6c929016fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatization:\n",
      "['After', 'a', 'messy', 'couple', 'of', 'month', ',', 'the', 'deal', 'between', 'Elon', 'Musk', 'and', 'Twitter', 'finally', 'closed', 'last', 'week', '.', 'And', 'a', 'his', 'first', 'order', 'of', 'business', ',', 'Elon', 'Musk', 'walked', 'into', 'the', 'company', 'with', 'a', 'physical', 'sink', '.', 'On', 'Twitter', ',', 'he', 'posted', 'a', 'video', 'of', 'this', 'stunt', 'with', 'the', 'caption', ':', '“', 'Entering', 'Twitter', 'HQ', '–', 'let', 'that', 'sink', 'in', '!', '”', 'And', 'just', 'like', 'that', ',', 'Musk', '’', 's', 'authority', 'over', 'the', 'popular', 'social', 'medium', 'platform', 'begin', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yuliia21\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization (using WordNet Lemmatizer)\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "print(\"\\nLemmatization:\")\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3dfc50-9f3f-4e31-b91f-f5316674d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "413027a8-307d-4fd1-afa2-1650cf6b3784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count of 'sink': 2\n"
     ]
    }
   ],
   "source": [
    "# Distinguishing the word \"sink\"\n",
    "sink_count = tokens.count(\"sink\")\n",
    "print(\"\\nCount of 'sink':\", sink_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ded1f-7b07-4c68-bf4b-8ab278384fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5151d9c7-880d-4d02-953c-4dbf30d2a380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\yuliia21\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yuliia21\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eb63067-1272-4988-af9b-abe307316fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part-of-Speech Tags:\n",
      "After: IN\n",
      "a: DT\n",
      "messy: JJ\n",
      "couple: NN\n",
      "of: IN\n",
      "months: NNS\n",
      ",: ,\n",
      "the: DT\n",
      "deal: NN\n",
      "between: IN\n",
      "Elon: NNP\n",
      "Musk: NNP\n",
      "and: CC\n",
      "Twitter: NNP\n",
      "finally: RB\n",
      "closed: VBD\n",
      "last: JJ\n",
      "week: NN\n",
      ".: .\n",
      "And: CC\n",
      "as: IN\n",
      "his: PRP$\n",
      "first: JJ\n",
      "order: NN\n",
      "of: IN\n",
      "business: NN\n",
      ",: ,\n",
      "Elon: NNP\n",
      "Musk: NNP\n",
      "walked: VBD\n",
      "into: IN\n",
      "the: DT\n",
      "company: NN\n",
      "with: IN\n",
      "a: DT\n",
      "physical: JJ\n",
      "sink: NN\n",
      ".: .\n",
      "On: IN\n",
      "Twitter: NN\n",
      ",: ,\n",
      "he: PRP\n",
      "posted: VBD\n",
      "a: DT\n",
      "video: NN\n",
      "of: IN\n",
      "this: DT\n",
      "stunt: NN\n",
      "with: IN\n",
      "the: DT\n",
      "caption: NN\n",
      ":: :\n",
      "“: NN\n",
      "Entering: NNP\n",
      "Twitter: NNP\n",
      "HQ: NNP\n",
      "–: NNP\n",
      "let: VBD\n",
      "that: IN\n",
      "sink: VB\n",
      "in: IN\n",
      "!: .\n",
      "”: NN\n",
      "And: CC\n",
      "just: RB\n",
      "like: IN\n",
      "that: DT\n",
      ",: ,\n",
      "Musk: NNP\n",
      "’: NNP\n",
      "s: JJ\n",
      "authority: NN\n",
      "over: IN\n",
      "the: DT\n",
      "popular: JJ\n",
      "social: JJ\n",
      "media: NNS\n",
      "platform: NN\n",
      "begins: VBZ\n",
      ".: .\n"
     ]
    }
   ],
   "source": [
    "# Perform Part-of-Speech Tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"\\nPart-of-Speech Tags:\")\n",
    "for word, pos in pos_tags:\n",
    "    print(f\"{word}: {pos}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51e1311d-b0c9-47a2-bce0-9b43cca72f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we can see that the word \"sink\" is tagged as \n",
    "#sink: NN and sink:VB\n",
    "#which makes it a correct part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06ff5a6-c4f4-4ebd-84f3-e3a202458a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
